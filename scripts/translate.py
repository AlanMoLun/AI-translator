# translate.py
# ----------------------------------------
# ä» glossary.pkl + glossary.index åŠ è½½è¯åº“
# ä½¿ç”¨å‘é‡åŒ¹é…æ‰¾åˆ°ç›¸å…³æœ¯è¯­
# å¹¶ç”¨ OpenAI ç¿»è¯‘è¾“å…¥å¥å­ï¼Œè‡ªåŠ¨å‚è€ƒæœ¯è¯­è¡¨

import os
import json
import faiss
import numpy as np
import pandas as pd
from openai import OpenAI

# å®šä¹‰ç›®å½•
projectRoot = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) + "\\"
binFolder = os.path.join(projectRoot, "bin") + "\\"
glossariesFolder = os.path.join(projectRoot, "glossaries") + "\\"
translationFolder = os.path.join(projectRoot, "translation") + "\\"

# =====================================================
# 1ï¸âƒ£ è¯»å–é…ç½®æ–‡ä»¶
# =====================================================
with open(projectRoot + "config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

with open(projectRoot + "local.json", "r") as f:
    local = json.load(f)

activeClient = config["activeClient"]
clientConfig = config[activeClient]
localConfig = local[activeClient]

# æ¨å¯¼æ–‡ä»¶å
pklFile = binFolder + "glossary.pkl"
indexFile = binFolder + "glossary.index"

# åˆå§‹åŒ– embedding å®¢æˆ·ç«¯
clientParams = {"api_key": localConfig["key"]}
if "base_url" in clientConfig:
    clientParams["base_url"] = clientConfig["base_url"]
if activeClient == "azure" and "api_version" in clientConfig:
    clientParams["api_version"] = clientConfig["api_version"]

client = OpenAI(**clientParams)

# åˆå§‹åŒ– chat å®¢æˆ·ç«¯
chatClientParams = {"api_key": localConfig["key"]}
if "base_url" in clientConfig:
    chatClientParams["base_url"] = clientConfig["base_url"]
if "api_version" in clientConfig:
    chatClientParams["api_version"] = clientConfig["api_version"]

chat_client = OpenAI(**chatClientParams)

# =====================================================
# 2ï¸âƒ£ åŠ è½½è¯åº“å’Œå‘é‡ç´¢å¼•
# =====================================================
try:
    glossary_data = pd.read_pickle(pklFile)
    print(f"âœ… æˆåŠŸåŠ è½½è¯åº“æ–‡ä»¶: {pklFile}")
    print(f"è¯åº“å…±æœ‰ {len(glossary_data)} æ¡è®°å½•")
except Exception as e:
    print(f"âŒ åŠ è½½ {pklFile} å¤±è´¥: {e}")
    exit(1)

try:
    index = faiss.read_index(indexFile)
    print(f"âœ… æˆåŠŸåŠ è½½ç´¢å¼•æ–‡ä»¶: {indexFile}")
except Exception as e:
    print(f"âŒ åŠ è½½ç´¢å¼•æ–‡ä»¶ {indexFile} å¤±è´¥: {e}")
    exit(1)

# =====================================================
# 3ï¸âƒ£ å®šä¹‰è¾…åŠ©å‡½æ•°
# =====================================================

# -------------------- é«˜äº®æœ¯è¯­ä½¿ç”¨æƒ…å†µï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰ --------------------
def get_terms_usage_string(translation: str, terms: list) -> str:
    """
    è¿”å›å‚è€ƒæœ¯è¯­ä½¿ç”¨æƒ…å†µçš„å­—ç¬¦ä¸²ã€‚
    æ”¯æŒå¤§å°å†™å¿½ç•¥å’Œç®€å•å˜å½¢åˆ¤æ–­ã€‚
    """
    lines = ["\nğŸ”¸ å‚è€ƒæœ¯è¯­ï¼š"]
    
    if not terms:
        lines.append("  ï¼ˆæ— åŒ¹é…æœ¯è¯­ï¼‰")
        return "\n".join(lines)

    translation_lower = translation.lower().replace("\n", " ").strip()

    for t in terms:
        term_trans = t["translation"].lower().strip()
        # å¿½ç•¥ç©ºç¿»è¯‘
        if not term_trans or term_trans in ("-", ""):
            used_flag = "âŒ æœªä½¿ç”¨"
        else:
            # æ‹†åˆ†æ–œæ å½¢å¼çš„å¤šé€‰ç¿»è¯‘
            options = [opt.strip() for opt in term_trans.split("/") if opt.strip()]
            used_flag = "âŒ æœªä½¿ç”¨"
            for opt in options:
                if opt in translation_lower:
                    used_flag = "âœ… å·²ä½¿ç”¨"
                    break

        lines.append(f"  {t['zh']} â†’ {t['translation']}ï¼ˆ{t['source_column']}ï¼‰ {used_flag}  [è·ç¦»: {t['distance']:.4f}]")

    return "\n".join(lines)


def get_embedding(text: str):
    """ç”Ÿæˆæ–‡æœ¬çš„ embedding å‘é‡"""
    resp = client.embeddings.create(
        model=clientConfig["model"],
        input=text
    )
    return np.array(resp.data[0].embedding, dtype="float32").reshape(1, -1)


def find_similar_terms(query: str, top_k: int = 3):
    """
    æŸ¥æ‰¾ä¸è¾“å…¥æ–‡æœ¬æœ€ç›¸ä¼¼çš„æœ¯è¯­ã€‚
    ä¼˜å…ˆä½¿ç”¨å­—ç¬¦ä¸²ç›´æ¥åŒ¹é…ï¼Œç„¶åå†ç”¨ embedding æœç´¢è¡¥å……ã€‚
    """
    matches = []

    # 1. ç›´æ¥åŒ¹é…ï¼šä¼˜å…ˆæ‰¾å‡ºå¥ä¸­æ˜ç¡®åŒ…å«çš„æœ¯è¯­
    for _, row in glossary_data.iterrows():
        zh_term = str(row["zh"]).strip()
        if zh_term and zh_term in query:
            matches.append({
                "zh": zh_term,
                "translation": row["selected_text"] if "selected_text" in row else row.get("en", ""),
                "source_column": row.get("source_column", "N/A"),
                "distance": 0.0
            })

    # 2. å‘é‡åŒ¹é…ï¼šè¡¥å……è¯­ä¹‰ä¸Šç›¸è¿‘çš„æœ¯è¯­
    try:
        query_emb = get_embedding(query)
        D, I = index.search(query_emb, top_k)
        for dist, idx in zip(D[0], I[0]):
            row = glossary_data.iloc[idx]
            zh_term = str(row["zh"]).strip()
            if zh_term not in [m["zh"] for m in matches]:
                matches.append({
                    "zh": zh_term,
                    "translation": row["selected_text"] if "selected_text" in row else row.get("en", ""),
                    "source_column": row.get("source_column", "N/A"),
                    "distance": float(dist)
                })
    except Exception as e:
        print(f"âš ï¸ å‘é‡æœç´¢å¤±è´¥: {e}")

    # 3. æŒ‰è·ç¦»æ’åºï¼ˆç›´æ¥åŒ¹é…çš„ distance=0 ä¼˜å…ˆï¼‰
    matches.sort(key=lambda x: x["distance"])
    return matches

def translate_with_glossary(query: str, auto_detect_terms: bool = True):
    """
    ç»“åˆ glossary ä¿¡æ¯çš„æ™ºèƒ½ç¿»è¯‘ã€‚
    åœ¨ç¿»è¯‘å‰æ•´åˆå­—ç¬¦ä¸²åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…ç»“æœã€‚
    """
    detected_terms = []
    
    # =====================================================
    # ğŸ§© å¯é€‰ï¼šä½¿ç”¨ AI è‡ªåŠ¨æ£€æµ‹ç–‘ä¼¼ä½›å­¦æœ¯è¯­
    # =====================================================
    if auto_detect_terms:
        try:
            term_resp = client.chat.completions.create(
                model=clientConfig["chatModel"],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä½›å­¦æœ¯è¯­è¯†åˆ«åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": f"åˆ—å‡ºä»¥ä¸‹å¥å­ä¸­å‡ºç°çš„ä½›å­¦æœ¯è¯­ï¼ˆåªåˆ—ä¸­æ–‡æœ¯è¯­ï¼‰ï¼š\n{query}"}
                ]
            )
            # æå–æ£€æµ‹åˆ°çš„æœ¯è¯­ï¼ˆç”¨ split() è‡ªåŠ¨å»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œï¼‰
            detected_terms = term_resp.choices[0].message.content.split()
            if detected_terms:
                print(f"ğŸ§  æ£€æµ‹åˆ°ç–‘ä¼¼æœ¯è¯­: {', '.join(detected_terms)}")
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹æœ¯è¯­æ—¶å‡ºé”™: {e}")
            detected_terms = []
    
    # =====================================================
    # ğŸ” æŸ¥æ‰¾ä¸è¾“å…¥æ–‡æœ¬æˆ–æ£€æµ‹æœ¯è¯­ç›¸ä¼¼çš„è¯æ±‡
    # =====================================================
    similar_terms = []
    if detected_terms:
        # å¯¹æ£€æµ‹åˆ°çš„æ¯ä¸ªæœ¯è¯­å•ç‹¬æŸ¥æ‰¾ç›¸ä¼¼é¡¹
        for term in detected_terms:
            similar_terms.extend(find_similar_terms(term, top_k=2))
    else:
        # å¦åˆ™åªæŸ¥æ•´å¥
        similar_terms = find_similar_terms(query)
    
    # å»é‡ï¼šæ ¹æ®ä¸­æ–‡æœ¯è¯­å»é‡
    seen = set()
    unique_terms = []
    for t in similar_terms:
        if t["zh"] not in seen:
            seen.add(t["zh"])
            unique_terms.append(t)

    # æ„å»ºæœ¯è¯­æç¤ºæ–‡æœ¬
    if similar_terms:
        glossary_context = "\n".join(
            [f"{t['zh']} â†’ {t['translation']}ï¼ˆ{t['source_column']}ï¼‰" for t in similar_terms]
        )
    else:
        glossary_context = "ï¼ˆæ— åŒ¹é…æœ¯è¯­ï¼‰"

    # æ„å»ºç¿»è¯‘ prompt
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä½›å­¦ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ä¸‹é¢çš„ä¸­æ–‡å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼Œ
å¹¶ä¸¥æ ¼å‚è€ƒä¸‹åˆ—æœ¯è¯­è¡¨ä¸­çš„è¯‘æ³•ã€‚
âš ï¸ è‹¥å¥ä¸­åŒ…å«çš„æœ¯è¯­å‡ºç°åœ¨æœ¯è¯­è¡¨ä¸­ï¼Œå¿…é¡»å®Œå…¨æŒ‰ç…§æœ¯è¯­è¡¨ç¿»è¯‘ã€‚

æœ¯è¯­è¡¨ï¼š
{glossary_context}

éœ€è¦ç¿»è¯‘çš„å¥å­ï¼š
{query}

è¯·è¾“å‡ºå¿ å®ã€è‡ªç„¶ã€ä¸“ä¸šçš„è‹±æ–‡ç¿»è¯‘ã€‚
"""

    resp = client.chat.completions.create(
        model=clientConfig["chatModel"],
        messages=[
            {"role": "system", "content": "You are a professional Buddhist translator."},
            {"role": "user", "content": prompt}
        ]
    )

    translation = resp.choices[0].message.content.strip()
    return translation, similar_terms

# =====================================================
# 4ï¸âƒ£ ä¸»ç¨‹åºå…¥å£
# =====================================================
if __name__ == "__main__":
    print("ğŸ“˜ AI ä½›å­¦æœ¯è¯­ç¿»è¯‘å™¨")
    print("è¾“å…¥ä¸­æ–‡å¥å­ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š")

    while True:
        query = input("\n> ").strip()
        if not query:
            continue
        if query.lower() in ("exit", "quit", "bye"):
            break

        translation, terms = translate_with_glossary(query, False)

        print("\nğŸ”¹ ç¿»è¯‘ç»“æœï¼š")
        print(translation)

        # print("\nğŸ”¸ å‚è€ƒæœ¯è¯­ï¼š")
        # for t in terms:
        #     print(f"  {t['zh']} â†’ {t['translation']} ({t['source_column']})  [è·ç¦»: {t['distance']:.4f}]")
        
        # é«˜äº®æœ¯è¯­ä½¿ç”¨æƒ…å†µ
        terms_str = get_terms_usage_string(translation, terms)
        print(terms_str)

